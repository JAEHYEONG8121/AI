{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN(Recurrent Neural Network)\n",
    "> 순환 신경망의 작동 원리에 대해 알기 전에, 먼저 순환 신경망의 필요성에 대해 살펴보자.\n",
    "\n",
    "![sequence1](img/sequence1.png)\n",
    "\n",
    "- 위의 사진을 보면, TRIANGLE과 INTEGRAL 이 두 글자는 같은 알파벳들의 나열이지만, 순서가 다르기 때문에 완전히 다른 의미를 갖는다.\n",
    "- 이와 같이 순서가 존재하는 데이터를 'Sequence데이터'라고 한다.\n",
    "- 똑같이 순서가 존재하지만 특별히 시간에 따른 의미가 존재하는 주가와 같은 데이터는 흔히 '시계열 데이터'라고 한다.\n",
    "- 결국, 순서가 존재하고 시간에 따라 다른 <font color = gold>이러한 데이터에서 숨은 패턴을 찾아냄으로써 어떠한 상관관계나 인과관계를 찾아내기 위해 고안된 모델이 바로 RNN이다.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**RNN 작동원리**\n",
    "\n",
    "![RNN1](img/RNN1.png)\n",
    "\n",
    "- 위의 그림은 RNN 모델을 도식화한 그림이다.\n",
    "- 우리가 앞서 봤던 CNN 모델과 어떤 차이가 있을까? 일반적인 CNN 모델과 RNN 모델을 비교하여 살펴보자.\n",
    "\n",
    "![RNN2](img/RNN2.png)\n",
    "\n",
    "![RNN3](img/RNN3.png)\n",
    "\n",
    "- 위의 첫 번째 그림은 일반적인 CNN이고, 두 번째 그림이 RNN을 나타내는 그림이다.\n",
    "- 두 그림의 차이점은 우리가 익숙한 CNN의 그림에 '순환성'을 추가했다는 점이다.\n",
    "- 위의 그림은 3개의 입력이 들어와서 4개의 노드로 구성된 은닉층 두개를 거치는데, 이를 노드 수를 모두 1개로 바꾸어서 그림처럼 표현하면 아래와 같아진다.\n",
    "\n",
    "![RNN4](img/RNN4.png)\n",
    "\n",
    "- 첫 번째 은닉층의 값이 다음번에 다시 해당 은닉층의 입력으로 들어가는 것을 볼 수 있다.\n",
    "- 매 시간마다 새로운 입력값이 들어오고 결과값이 계산된다고 하면, t=0일때 이미 한 번 값들이 계산될 테고 그러면 t=1일 때는 이전 시간(t=0)의 은닉층 값과 현재 시간(t=1)의 입력값의 조합으로 값이 계산된다고 할 수 있다.\n",
    "\n",
    "![RNN5](img/RNN5.png)  ![RNN6](img/RNN6.png)  ![RNN7](img/RNN7.png)\n",
    "\n",
    "- 'pytorch'알파벳으로 예를 들자면, p라는 단어가 들어왔었다는 것을 기억하고 있는 상태로 이번에 y가 들어왔을 때 다음에 나와야 할 알파벳이 무엇인지 예측하는 방식이라고 표현할 수 있다.\n",
    "- 이러한 경우 앞에 들어왔던 입력값에 대한 정보가 없다면 어떠한 맥락도 모르기 때문에 제대로 예측할 수 없을 것이다.\n",
    "\n",
    "- 즉, 정리하자면 다음과 같다. 은닉층의 노드들은 어떠한 초기값을 가지고 있고, 첫 번째 입력값이 들어온 시점에서 입력값과 초기값을 조합으로 은닉층의 값들이 계산된다. 이 시점에서 결과값이 도출되면, 그 다음 시점에서는 새로 들어온 입력값과 그 전 시점에서 계산된 은닉층의 값과의 조합으로 은닉층의 값과 결과값이 다시 계산된다.\n",
    "\n",
    "- RNN은 입력과 출력의 길이를 다르게 설계할 수 있어 다양한 용도로 사용할 수 있는 장점이 있다.\n",
    "\n",
    "***\n",
    "1. 일 대 다(one-to-many)\n",
    "- 하나의 입력에 대해 여러 개의 출력을 가지는 일 대 다 모델의 경우, 하나의 이미지 입력에 대해서 사진의 제목을 출력하는 '이미지 캡셔닝'에 사용할 수 있다.\n",
    "- 사진의 제목이 단어들의 나열이므로 시퀸스 출력이다.\n",
    "\n",
    "![RNN8](img/RNN8.png)\n",
    "\n",
    "2. 다 대 일(many-to-one)\n",
    "- 시퀸스 입력에 대해서 하나의 출력을 하는 다 대 일 모델은 입력 문서가 긍정적인지 부정적인지 판별하는 '감성 분류'나 정상 메일인지 스팸 메일인지 분류하는 '스팸 메일 분류'에 사용할 수 있다.\n",
    "\n",
    "![RNN9](img/RNN9.png)\n",
    "\n",
    "3. 다 대 다(many-to-many)\n",
    "- 시퀸스 입력에 대해 시퀸스 출력을 하는 다 대 다 모델의 경우 '챗봇'이나 '번역기'에 많이 쓰인다.\n",
    "- '개체명 인식'이나 '품사 태깅'과 같은 작업 또한 다 대 다 모델에 속한다.\n",
    "\n",
    "![RNN10](img/RNN10.png)\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN의 학습과정**\n",
    "- 일정 시간 동안 모든 값이 계산되면, 모델을 학습하기 위해 결과값과 목표값의 차이를 손실 함수를 통해 계산하고 역전파해야 하는데, 기존의 역전파와 다르게 RNN에서는 계산에 사용된 시점의 수의 영향을 받는다.\n",
    "- 예를 들어, t=0에서 t=2까지 계산에 사용됐다면 그 시간 전체에 대해 역전파를 해야 하는 것이다.\n",
    "- 이를 <font color=gold>시간에 따른 역전파</font>라고 부른다.\n",
    "\n",
    "![RNN12](img/RNN12.png)\n",
    "\n",
    "- 위의 그림을 보면 t가 0, 1, 2인 시점에서 각각 결과값이 나오고 목표값과 비교되는 것을 볼 수 있다.\n",
    "- 다시 단어 pytorch를 예로 들면, t=0에서 입력값으로 p가 들어올 것이다. 우리는 t=0일 때 결과값으로 y가 나오길 기대하기 때문에 target_0에는 y가 들ㅇ가고, 결과가 y와 같지 않다면 손실이 생길 것이다.\n",
    "- t=1에서 y가 들어가면 이번에는 target_1은 t이고, t=2에서는 입력값이 t, target_2는 o가 된다.\n",
    "\n",
    "- 이때 모델을 학습하려면 어떻게 해야 할까?\n",
    "- t=2의 시점에서 발생한 손실을 역전파하기 위해서는 손실을 입력과 은닉층들 사이의 가중치로 미분하여 손실에 대한 각각의 비중을 구해 업데이트하면 된다.\n",
    "- 하지만, 이 연산 과정에서 은닉층의 이전 시점의 값들이 연산에 포함되게 되는데 이전 시점의 값들은 내부적으로 다시 가중치, 입력값, 이전 시점의 값들의 조합으로 이루어져 있다.\n",
    "- RNN은 각 위치별로 같은 가중치를 공유하므로 t=2 시점의 손실을 역전파하기 위해서는 결과적으로 t=0시점의 노드 값들에도 다 영향을 주어야 한다.\n",
    "- 즉, 시간을 역으로 거슬러 올라가는 방식으로 각 가중치들을 업데이트해야 하기 때문에 시간에 따른 역전파라는 이름이 붙은 것이다!\n",
    "\n",
    "![RNN13](img/RNN13.png)\n",
    "\n",
    "![RNN14](img/RNN14.png)\n",
    "\n",
    "- 앞의 그림에 가중치들을 표시하면 첫 번째 그림과 같다. t=2 시점만 떼어내 수식을 살펴보면 아래와 같다.\n",
    "\n",
    "![RNN](img/RNN11.png.jpg)\n",
    "\n",
    "- 여기서 w22에 대한 기울기를 보면 h2_in을 미분하는 부분이 있는데, 이를 계산해보면 h2_t=1이 나온다.\n",
    "- 그런데 사실 h2_t=1 값은 이전 시점의 값들의 조합으로 이루어져 있고 내부적으로도 w22를 포함하고 있기 때문에 이 값을 제대로 미분하기 위해서는 t=0 시점까지 계속 미분을 해야 한다.\n",
    "- 시점별로 정리해보자\n",
    "    - t=2 시점에서 발생한 손실은 t=2, 1, 0 시점에 전부 영향을 준다.\n",
    "    - t=1 시점의 손실은 t=1, 0에 영향을 주고\n",
    "    - t=0 시점의 손실은 t=0의 가중치에 영향을 준다.\n",
    "- 실제로 업데이트할 때는 가중치에 대해 시점별 기울기를 다 더해서 한 번에 업데이트를 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52d90d3cc821dd0beedd6e719dbdecc722c226b9d90ed1b663c34e1877f1142e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
