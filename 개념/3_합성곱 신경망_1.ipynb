{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 합성곱 신경망(Convolutional Neural Networks)\n",
    "***합성곱 신경망 발달 배경**\n",
    "> 이론상으로 인공 신경망은 실수 공간에서 연속하는 모든 함수를 근사할 수 있다.(보편 근사 정리) 하지만, 무한히 많은 뉴런을 사용하는 네트워크는 만들 수도 없고 만들더라도 학습 과정에서 여러문제가 있을 것이다.  \n",
    "\n",
    "> 앞에서 다루었던 인공 신경망 모델을 사용하여 이미지 데이터에 적용해보면 성능이 잘 나오지 않는다. 실제 예시를 통해 알아보자."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 합성곱 신경망은 이미지 처리에 탁월한 성능을 보인다.\n",
    "- 예를 들어, 알파벳 손글씨를 분류하는 문제가 있다고 생각해보자. 아래 그림은 알파벳 Y를 비교적 정자로 쓴 손글씨와 다소 휘갈겨 쓴 손글씨 두개를 2차원 텐서인 행렬로 표현한 그림이다.\n",
    "\n",
    "![Y](img/Y.png)\n",
    "\n",
    "- 사람이 보기에는 두 그림 모두 알파벳 Y로 손쉽게 판단이 가능하지만, 컴퓨터가 보기에는 각 픽셀마다 가진 값이 대부분 다르기 때문에 사실상 다른 값을 가진 입력으로 판단한다.\n",
    "- 하지만, 이미지라는 것은 위의 경우와 같이 같은 대상이라도 휘어지거나, 이동되었거나, 방향이 뒤틀렸거나 등 다양한 변형이 존재한다.\n",
    "- 다층 퍼셉트론(Perceptron = 인공 뉴런)은 몇 가지 픽셀만 값이 달라져도 민감하게 예측에 영향을 받는다는 단점이 존재한다.\n",
    "\n",
    "![Y2](img/Y2.png)\n",
    "\n",
    "\n",
    "- 좀 더 구체적으로 살펴보자. 위 손글씨를 다층 퍼셉트론으로 분류한다고 하면, 이미지를 1차원 텐서인 벡터로 변환하고 다층 퍼셉트론의 입력층으로 사용해야 한다.\n",
    "- 위의 그림과 같이 1차원으로 변환된 결과는 사람이 보기에도 원본 이미지가 어떤 이미지였는지 알아보기가 어렵다. \n",
    "- 컴퓨터도 마찬가지다. 원본 이미지가 갖고 있던 '공간적인 구조(spatial structure)'정보가 유실되었기 때문이다.\n",
    "- 결국 이미지의 공간적인 구조 정보를 보존하면서 학습할 수 있는 방법이 필요해졌고, 이를 위하여 CNN이 등장하게 된다. </br>\n",
    "-> CNN의 중요 포인트는 이미지 전체보다는 부분을 보는 것, 그리고 이미지의 한 픽셀과 주변 픽셀들의 연관성을 살리는 것이다!\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 합성곱 연산 과정\n",
    "***이미지**\n",
    "> 합성곱 연산 과정을 살펴보기 전에 이미지에 대한 기본적인 용어들을 살펴보자.\n",
    "\n",
    "- 이미지는 '(높이, 너비, 채널)'이라는 3차원 텐서이다.\n",
    "    - 높이 : 이미지의 세로 방향 픽셀 수\n",
    "    - 너비 : 이미지의 가로 방향 픽셀 수\n",
    "    - 채널 : 색 성분\n",
    "        - 흑백 이미지의 채널 수 = 1\n",
    "        - 컬러 이미지의 채널 수 = 3(RGB)</br></br>\n",
    "- 위 손글씨 데이터는 흑백 이미지므로 채널 수가 1임을 고려하면 (28 x 28 x 1)의 크기를 갖는 3차원 텐서이다.\n",
    "\n",
    "- 흑백이 아닌 컬러 이미지일 경우 RGB에 의해 채널 수가 3이다. 하나의 픽셀은 삼원색의 조합으로 이루어진다.\n",
    "- 위 손글씨 데이터가 컬러였다면 (28 x 28 x 3)의 크기를 갖는 3차원 텐서인 것이다.\n",
    "- 채널은 때로는 '깊이(depth)'라고도 한다. 이 경우 이미지는 '(높이, 너비, 깊이)'라는 3차원 텐서로 표현된다고 말할 수 있을 것이다.\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***합성곱 연산 과정**\n",
    "> 이미지에 대한 기본 정보를 바탕으로 합성곱 연산에 대해 이해해보자.\n",
    "\n",
    "- 합성곱층은 합성곱 연산을 통해 <font color = blue>이미지의 특징을 추출</font>하는 역할을 한다. 즉, 우리의 입력값 이미지의 모든 영역에 같은 필터를 반복 적용해 패턴을 찾아 처리하는 것이 CNN의 목적이다!</br></br>\n",
    "- 합성곱(Convolution)연산은 '커널(kernel) or 필터(filter)'라는 (n x m)크기의 행렬로 (높이 x 너비) 크기의 이미지를 처음부터 끝까지 겹치며 훑으면서 (n x m)크기의 겹쳐지는 부분의 각 이미지와 커널의 원소의 값을 곱해서 모두 더한 값을 출력으로 하는 것을 말한다.</br></br>\n",
    "- 커널은 일반적으로 3 x 3 or 5 x 5를 사용한다!\n",
    "- 이때, 이미지의 가장 왼쪽 위부터 가장 오른쪽 아래까지 순차적으로 훑는다 </br></br>\n",
    "- 결국 합성곱 연산은 각 성분끼리의 곱의 합 -> 내적이라고 생각하면 된다! ->> 내적의 의미가 무엇인가? : 내적값이 클수록 '연관성'이 크다!!\n",
    "\n",
    "![CNN](img/CNN.png)\n",
    "\n",
    "- 위의 그림은 3x3크기의 커널로 5x5의 이미지 행렬에 합성곱 연산을 수행하는 과정이다.\n",
    "- 위와 같은 연산을 9번 진행되어 마치면 최종 결과는 3x3크기의 행렬일 것이다.\n",
    "\n",
    "![CNN2](img/CNN3.png)\n",
    "\n",
    "- 위와 같이 입력으로부터 커널을 사용하여 합성곱 연산을 통해 나온 결과를 '특성 맵(feature map)'(=활성화 지도)이라고 한다.\n",
    "\n",
    "- 위의 예제에서는 커널의 크기가 3x3이었고, 커널의 이동 범위가 한 칸이었다.\n",
    "\n",
    "- 커널의 이동범위를 '스트라이드(stride)'라고 하는데, 스트라이드에 따라서 합성곱 연산의 결과인 특성 맵의 크기가 달라질 수 있다.\n",
    "\n",
    "![CNN4](img/CNN4.png)\n",
    "\n",
    "- 위의 그림을 통해 stride = 2로 바꾸었을 때 도출되는 특성 맵의 크기가 3x3에서 2x2로 바뀌었음을 알 수 있다!\n",
    "\n",
    "- 입력 이미지의 크기 : I, 필터의 크기 : K, 스트라이드 : S라고 하면 활성화 지도 O의 크기는 아래 식과 같다.\n",
    "\n",
    "![활성화 지도](img/Feature%20map.png)\n",
    "***\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***합성곱 신경망 vs 인공 신경망 비교**\n",
    "> 합성곱 신경망 역시 신경망이기 때문에 인공 신경망의 형태로도 연산을 표현할 수 있다.  \n",
    "\n",
    "> 간단한 예시로 3x3입력 이미지에 2x2필터, 스트라이드 1을 적용해보면 아래의 그림처럼 표현할 수 있다.\n",
    "\n",
    "![CNN5](img/CNN5.png)\n",
    "\n",
    "- 합성곱 연산도 인공 신경망의 일종이라고 볼 수 있다.\n",
    "\n",
    "- 대신 하나의 결과값이 생성될 때 입력값 전체가 들어가지 않고, 필터가 지나가는 부분만 연산에 포함된다.\n",
    "\n",
    "- 하나의 이미지에 같은 필터를 연달아 적용하기 때문에 가중치가 공유되어 기본 인공 신경망보다 학습의 대상이 되는 변수가 적다.\n",
    "\n",
    "- 즉, 합성곱 신경망은 다층 퍼셉트론을 사용할 때보다 훨씬 적은 수의 가중치를 사용하며 공간적 구조 정보를 보존하게 되는것이다!\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***패딩(Padding)**\n",
    "> 위의 예시에서 5x5이미지에 3x3의 커널로 합성곱 연산을 했을 때, 스트라이드가 1인 경우에는 3x3의 특성 맵을 얻었다.  \n",
    "\n",
    "> 이와 같이 합성곱 연산의 결과로 얻은 특성 맵은 입력보다 크기가 작아진다는 특징이 존재한다.  \n",
    "\n",
    "> 합성곱 층을 여러개 쌓았다면 최종적으로 얻은 특성 맵은 초기 입력보다 매우 작아진 상태가 되어버린다. 합성곱 연산 이후에도 특성 맵의 크기가 입력의 크기와 동일하게 유지되도록 하고 싶다면 '패딩'을 사용하면 된다.\n",
    "\n",
    "![패딩](img/Padding2.png)\n",
    "\n",
    "- 패딩은 합성곱 연산을 하기 전에, 입력의 가장자리에 지정된 개수의 폭만큼 행과 열을 추가해주는 것을 의미한다.\n",
    "- 주로 값을 0으로 채우는 '제로 패딩(zero padding)'을 사용한다.\n",
    "\n",
    "- 위의 그림처럼 1폭짜리 제로 패딩을 사용하여 위아래에 하나의 행을, 좌우에 하나의 열을 추가한 모습이다.\n",
    "\n",
    "- 이렇게 패딩한 후에 3x3커널을 이용하여 합성곱 연산을 하게되면 특성 맵이 기존의 입력 이미지의 크기와 같은 5x5의 크기를 갖는 것을 알 수 있다.\n",
    "\n",
    "- 즉, 패딩을 사용함으로써 입력 이미지에서 충분한 특성을 뽑아낼 수 있게 된다.\n",
    "\n",
    "- 입력 이미지의 크기 : I, 필터의 크기 : K, 스트라이드 : S, 패딩의 크기를 P라고 하면 활성화 지도 O의 크기는 아래 식과 같다.\n",
    "\n",
    "![패딩3](img/Padding3.png)\n",
    "***\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***풀링(Pooling)**\n",
    "> 입력 이미지가 엄청나게 큰 이미지라면 굳이 패딩을 적용하지 않아도 충분히 특성들을 뽑아낼 수 있고, 패딩 없이 여러 은닉층을 거쳐도 활성화 지도의 크기는 여전히 클 것이다.  \n",
    "\n",
    "> 상황에 따라서 엄청나게 높은 화질이 필요하지 않을 수도 있고, 좀 넓게 봐야 파악할 수 있는 특성도 있을 것이다. 이럴 때, 활성화 지도를 '다운샘플링or서브샘플링'하여 활성화 지도의 크기를 줄이는 연산이 바로 풀링이다\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 풀링은 합성곱 신경망에서 크게 두 종류가 사용된다.\n",
    "1. 맥스 풀링\n",
    "    - 일정 크기의 구간 내에서 가장 큰 값만을 전달하고 다른 정보는 버리는 방법\n",
    "    - 합성곱 연산의 관점에서 일정 구간에서 해당 필터의 모양과 가장 비슷한 부분을 전달하는 연산이라고 할 수 있다. </br></br>\n",
    "\n",
    "2. 평균 풀링\n",
    "    - 일정 크기의 구간 내의 값들의 평균을 전달하는 방법\n",
    "    - 합성곱 연산의 관점에서 해당 필터의 모양과 평균적으로 얼마나 일치하는지를 뽑아낸다고 할 수 있다.\n",
    "\n",
    "![풀링](img/Pooling.png)\n",
    "\n",
    "- 합성곱 신경망의 일반적인 구조를 살펴보면, 입력값에 대해 몇 번의 합성곱 연산을 활성 함수와 함께 적용한 이후 풀링으로 전체 크기를 줄여주는 과정을 반복한다.\n",
    "\n",
    "- 어느 정도 특성들을 다 뽑은 이후에는 뽑은 특성들을 입력으로 받는 인공 신경망을 뒤에 붙여서 각 클래스별 확률을 뽑아내거나(분류 문제), 특정 수치들을 뽑아낸다(회귀 문제). 이를 그림으로 표현하면 아래와 같다.\n",
    "\n",
    "![풀링2](img/Pooling2.png)\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***모델의 3차원적 이해**\n",
    "> 지금까지 이미지의 '채널or깊이'를 고려하지 않고 2차원 텐서를 가정하고 설명했다. 합성곱 신경망을 더 쉽게 이해하려면 3차원적으로 생각해보는 것이 좋다.\n",
    "\n",
    "- 입력 이미지를 컬러 이미지라고 가정하면 R, G, B의 3가지 채널로 구성되어 있기 때문에 3차원의 크기를 갖는다. 이러한 모양을 order-3 텐서라고 칭한다.\n",
    "\n",
    "![3차원](img/3D.png)\n",
    "\n",
    "- 위의 이미지와 같은 경우에 필터가 3x3x3의 크기를 갖는 order-3 텐서라면, 결과값의 모양은 (d1 - 2)x(d2 - 2)가 되는 것이 당연하다.\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CNN의 전체적인 네트워크 구조**\n",
    "> CNN의 구조는 기존의 완전연결계층(Fully-Connected Layer = Dense Layer)과는 다르게 구성되어 있다. 완전연결계층에서는 이전 계층의 모든 뉴런과 결합되어있는 Affine계층으로 구현했지만, CNN은 Convolutional Layer와 Pooling Layer들을 활성화 함수 앞뒤에 배치하여 만들어진다.\n",
    "\n",
    "1. 첫번째 Convolutional Layer\n",
    "\n",
    "![첫번째](img/CNN6.png)\n",
    "\n",
    "- 우리에게 주어진 입력값은 28x28 크기를 갖는 이미지이다. 이 이미지를 대상으로 여러개의 필터(커널)을 사용하여 결과값을 얻는다.\n",
    "\n",
    "- 한 개의 28x28 이미지 입력값에 10개의 5x5 필터를 사용하여 10의 24x24 matrics, 즉 convolution 결과값을 만들어 냈다.\n",
    "\n",
    "- 그 후 이렇게 도출된 결과값에 활성화 함수(ex : ReLU function)을 적용한다. 이렇게 첫번째 Convolutional Layer가 완성되었다.\n",
    "\n",
    "- 위의 과정을 통해 한 Convolutional Layer는 Convolution처리와 Activation function으로 구성되어 있다는 것을 알 수 있다.\n",
    "\n",
    "   <font color = gold>A Convolutional Layer = convolution + activation function</font>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 첫번째 Pooling Layer\n",
    "\n",
    "![PL](img/CNN7.png)\n",
    "\n",
    "- 이전 단계에서 convolution연산을 통해 많은 수의 결과값들을 생성하였다. 하지만, 위와 같이 한 개의 이미지에서 10개의 이미지 결과값이 도출되어버리면 값이 너무 많아지는것이 문제가 된다.\n",
    "\n",
    "- 그렇기 때문에 Pooling을 통해서 각 결과값인 특성 맵의 차원을 축소시켜준다. 즉, correlation이 낮은 부분을 삭제하여 결과값의 크기(=차원)을 줄이는 과정을 하는 것이다.\n",
    "\n",
    "- 위의 그림에서 2x2 크기로 Max Pooling 하는 것으로 그려져있으므로 24x24 에서 2x2 크기의 matrix에서 가장 큰 값을 가져와서 결과값의 크기를 반으로 줄여준다. -> 12x12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 두번째 Convolutional Layer\n",
    "\n",
    "![CNN8](img/CNN8.png)\n",
    "\n",
    "- 이번 Convolutional Layer에서는 텐서 convolution을 적용한다.\n",
    "\n",
    "- 이전의 Pooling Layer에서 얻어낸 12x12x10 텐서를 대상으로 5x5x10 크기의 텐서필터 20개를 사용해준다. -> 8x8 크기의 결과값 20개를 얻어낼 수 있다!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 두번째 Pooling Layer\n",
    "\n",
    "![CNN9](img/CNN9.png)\n",
    "\n",
    "- 이전과 같은 방식으로 2x2 Max Pooling을 해주면 4x4 크기의 결과값 20개를 얻는다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Flatten(Vectorization)\n",
    "\n",
    "![CNN10](img/CNN10.png)\n",
    "\n",
    "- 위의 과정을 거친 후 4x4x20 크기의 텐서를 일자 형태의 데이터로 쭉 펼친다고 생각해보자.\n",
    "\n",
    "- 이 과정을 Flatten 또는 Vectorization이라고 한다. 쉽게 말해 각 세로줄을 일렬로 쭉 세워두는 것이다. 그러면 이것은 320-dimension을 가진 벡터 형태가 되는 것이다.\n",
    "\n",
    "- 그렇다면 왜 이렇게 1차원 데이터로 변형해도 되는 걸까?</br>\n",
    "-> 이 전에 두번째 Pooling Layer에서 얻어낸 4x4크기의 이미지들은 이미지 자체라기 보다는 입력된 이미지에서 얻어온 특이점 데이터가 된다. 즉, 1차원의 벡터 데이터로 변형시켜주어도 무관한 상태가 된다는 것이다!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Fully-Connected Layers(Dense Layers)\n",
    "\n",
    "![CNN11](img/CNN11.png)\n",
    "\n",
    "- 이제 마지막으로 하나 혹은 하나 이상의 Fully-Connected Layer를 적용시키고 마지막에 Softmax activation function을 적용해주면 최종 결과물을 출력하게 된다!\n",
    "\n",
    "- cf) 완전연결계층\n",
    "    - '완전 연결 되었다'는 뜻은 한 층(layer)의 모든 뉴런이 그 다음 층(layer)의 모든 뉴런과 연결된 상태를 말한다.\n",
    "    - 1차원 배열의 형태로 평탄화된 행렬을 통해 이미지를 분류하는데 사용되는 계층이다.\n",
    "    - Flatten을 통해 3차원 벡터의 텐서를 1차원 배열로 평탄화하고, ReLU함수로 뉴런을 활성화하고 softmax함수로 이미지를 분류하는 것까지가 Fully-Connected Layer라고 할 수 있다.\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***소프트맥스 함수(Softmax function)**\n",
    "> 소프트맥스 함수는 출력층에서 사용되는 함수이다. 특히 '다중 클래스 분류 모델'을 만들 때 사용한다. 결과를 확률로 해석할 수 있게 변환해주는 함수로 높은 확률을 가지는 class로 분류한다. 이는 결과값을 정규화시키는 것으로도 생각할 수 있다.\n",
    "\n",
    "![sm1](img/softmax.png)\n",
    "\n",
    "- 소프트맥스 함수식은 위와 같다. 구체적인 예시를 통해 소프트맥스 함수가 어떻게 사용되는지 알아보자.\n",
    "\n",
    "![sm2](img/softmax2.png)\n",
    "\n",
    "- 해당 이미지가 강아지인지 고양이인지 구분하는 문제를 푼다고 했을 때 인공신경망의 결과는 위의 그림처럼 특정 수치를 결과값을 도출하게 된다.\n",
    "\n",
    "- 우리가 원하는 결과는 '고양이'라는 정답인데 어떻게 하면 인공 신경망의 결과값을 정답과 비교하여 오류를 계산하고 역전파를 통해 모델을 학습시킬 수 있을지 살펴보자.\n",
    "\n",
    "1. 우선 정답을 구분하려는 클래스의 개수에 맞게 바꿔야 한다.\n",
    "    - 클래스의 개수가 2개이고 강아지, 고양이 순으로 모델의 결과값이 도출된다면 고양이라는 정답은 [0, 1]의 벡터로, 강아지는 [1, 0]으로 변화시켜 개수를 맞추어 주어야 한다.\n",
    "    - 이렇게 하나만 1이고 나머지는 0인 형태를 '원-핫(one-hot)'벡터라고 하고 이렇게 변환시키는 과정을 '원-핫 인코딩(one-hot encoding)'이라고 부른다.\n",
    "    - 즉, 원-핫 인코딩은 본래의 정답을 확률분포로 변환해주는 것이라고 볼 수 있다. 정답에 해당하는 확률은 1, 나머지는 다 0이 된다. </br></br>\n",
    "\n",
    "2. 소프트맥스 함수 적용\n",
    "    - 1번의 과정을 거쳐도 비교 가능한 상태는 아니다. 신경망의 결과값이 확률이 아니기 때문이다.\n",
    "    - 신경망의 결과값을 확률로 바꾸어주기 위하여 소프트맥스 함수를 적용한다.\n",
    "    - 위의 예시의 결과값 벡터인 [0.37, 1.58]은 지수함수를 거쳐 [e^0.37, e^1.58]=[1.4477, 4.8549]로 변환되고 전체 합 중 각각의 비중이 소프트맥스 함수의 결과가 된다.\n",
    "    - 최종적으로 신경망의 결과값은 [0.2296, 0.7704]가 된다.</br></br>\n",
    "\n",
    "3. 손실 측정\n",
    "    - 클래스를 구분하는 작업에서는 주로 '교차 엔트로피(cross entropy)' 손실 함수를 사용한다.\n",
    "    - 엔트로피 : 정보량의 기댓값</br>\n",
    "    \n",
    "    ![entropy](img/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC.png)\n",
    "    - 만약 p(x)가 일어날 확률이 낮아 작은 값을 가진다면 -logP(x)값은 커지게 된다. \n",
    "    - 즉, 일어날 확률이 작을수록 가지고 있는 정보가 크고, 일어날 확률이 클수록 가지고 있는 정보가 작은 것이다.\n",
    "    - ex) 확률이 반반인 동전 던지기는 H(X) = 0.5(-log(0.5)) + 0.5(-log(0.5)) = 1이되어 하나의 비트로 표현이 가능해진다.</br></br>\n",
    "    - 만약 누군가가 동전 던지기의 확률이 원래 각각 1/2씩인데 누군가가 1/4, 3/4라고 잘못 예측했을 때 분포의 차이에 의해 생기는 추가적 손실은 어떻게 구할 수 있을까?\n",
    "    - 위와 같은 경우에 사용되는 개념이 <font color=gold>'교차 엔트로피'</font>이다.</br>\n",
    "    \n",
    "    ![entropy2](img/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC2.png)\n",
    "    - 교차 엔트로피는 목표로 하는 최적의 확률분포 p와 이를 근사하려는 확률분포 q가 얼마나 다른지를 측정하는 방법이다.\n",
    "    - 즉, 교차 엔트로피는 원래 p였던 분포를 q로 표현했을 때 얼마만큼의 비용이 드는지를 측정한다고 할 수 있다.\n",
    "    - 앞에서 동전 던지기의 경우에서 교차 엔트로피를 계산해보면 H(p, q) = 0.5(-log(0.25)) + 0.5(-log(0.75)) = 1 + 0.2075 = 1.2075가 되고 이는 기존의 엔트로피 1보다 크다. 즉, 최적인 상태보다 더 많은 비트가 필요하고 그만큼의 손해가 발생하는 셈이다.</br>\n",
    "\n",
    "    ![entropy3](img/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC3.png)\n",
    "    - 식을 전개해서 위와 같이 표현하기도 한다.\n",
    "    - 즉 교차 엔트로피는 최적의 분포 p의 엔트로피에 KLD항을 더한 것으로 표현할 수 있다.\n",
    "    - KLD는 'Kullback-Leibler Divergence'의 줄임말로, 분포 p를 기준으로 q가 얼마나 다른지를 측정하는 방법이고, 따라서 교차 엔트로피를 최소화한다는 것은 <font color = gold>p의 엔트로피는 고정된 값이므로 KLD를 최소화하여 q가 p의 분포와 최대한 같아지게 한다는 의미이다!!</font> </br></br>\n",
    "\n",
    "    - 마지막 예시로 강아지인지 고양이인지 판별하는 예시로 돌아가서, 실제 정답이 고양이인데 강아지, 고양이 순으로 [0.1, 0.9]의 소프트맥스 결과가 나왔다고 가정하자.\n",
    "    - 이때 교차 엔트로피 손실은 (-[0xlog(0.1) + 1xlog(0.9)]) = 0.1053이 나오고, L1 손실은 1 - 0.9 = 0.1이 나온다.\n",
    "    - 별 차이가 없는 것처럼 보이기도 하지만 예측을 더 못했을 경우를 생각해보자. 고양이를 0.3으로 예측했다면 교차 엔트로피 손실은 (-[0xlog(0.7) + 1xlog(0.3)]) = 1.2039이고, L1 손실은 1 - 0.3 = 0.7이 나온다.\n",
    "    - 혹은 더 예측을 못해서 고양이를 0.1로 예측했다면 교차 엔트로피 손실은 (-[0xlog(0.9) + 1xlog(0.1)]) = 2.3025, L1 손실은 1 - 0.1 = 0.9이다.</br>\n",
    "    - 즉, 교차 엔트로피 값은 예측이 잘못될수록 L1 손실보다 더 크게 증가하는 것을 확인할 수 있다. 그만큼 더 페널티가 크고 손실 값이 크기 때문에 학습 면에서도 교차 엔트로피 손실을 사용하는 것이 장점이 있다는 뜻이다! -> '분류'문제에서는 교차 엔트로피 손실을 많이 사용한다!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***학습 가능한 매개변수의 수**\n",
    "> 이제 위에서 살펴본 CNN모델의 학습 가능한 매개변수(trainable parameters)는 몇개나 되는지 살펴보자.\n",
    "\n",
    "![CNN12](img/CNN12.png)\n",
    "\n",
    "- 첫 번째 convolution과정에서 5x5크기의 필터 10개를 사용하였으므로 5x5x10 = 250개의 매개변수가 존재한다.\n",
    "- Pooling Layer에서는 단순히 크기를 줄이는 개념이므로 매개변수가 없다.\n",
    "- 두 번째 convolution과정에서 5x5x10크기의 텐서 20개를 사용하였으므로 5x5x10x20 = 5000개의 매개변수가 추가되었다.\n",
    "- 두 개의 Fully-connected layer들에서 각각 매개변수 32000개, 1000개가 추가되었다.</br>\n",
    "-> 결과적으로 생략한 intercepts값까지 고려해보면 총 38250개 이상의 학습 가능한 매개변수를 갖게 된다!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***초매개변수(Hyper-parameters)**\n",
    "> 마지막으로 CNN모델에서 튜닝 가능한 하이퍼파라미터는 어떤 것들이 있는지 살펴보자. 하이퍼파라미터란 학습의 대상이 아니라 학습 이전에 정해놓는 변수를 의미한다.\n",
    "\n",
    "1. Convolutional Layers : 필터의 갯수, 필터의 크기, stride값, zero-padding의 유무\n",
    "2. Pooling Layers : pooling방식 선택(MaxPool or AvgPool), Pool의 크기, Pool stride값\n",
    "3. Fully-connected Layers : 넓이(width)\n",
    "4. 활성함수 : ReLU(가장 주로 사용됨), SoftMax(multi class classification), Sigmoid(binary classification)\n",
    "5. Loss fuction : Cross-entropy for classification, L1 or L2 Loss for regression\n",
    "6. 최적화(Optimization) 알고리즘과 이것에 대한 하이퍼파라미터(보통 learning rate) : SGD, SGD with momentum, AdaGrad, RMSprop\n",
    "7. Random initialization : Gaussian or uniform, Scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***참고문헌***</br>\n",
    "-https://halfundecided.medium.com/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-cnn-convolutional-neural-networks-%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-836869f88375\n",
    "\n",
    "-파이토치 첫걸음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52d90d3cc821dd0beedd6e719dbdecc722c226b9d90ed1b663c34e1877f1142e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
